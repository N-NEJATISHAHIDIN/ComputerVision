{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5: Photometric Stereo and Structure from Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5.1 Lighting a Scene\n",
    "\n",
    "In class we talked about the properties of light. One of the topics we discussed was how we could compute the scene lighting of a Lambertian object from its normals and its albedo (and assuming we know where the light is). This process is relatively \"simple\" once you understand what's going on. For a Lambertian object, the amount of light collected by a light source is proportional to the cosine of the angle between the surface and the light source. So, for every point on the surface $\\mathbf{x}$:\n",
    "\n",
    "$$ \\text{Observed Brightness}(\\mathbf{x}) = \\text{Albedo}(\\mathbf{x})\\cos(\\theta_\\mathbf{x}) = \\text{Albedo}(\\mathbf{x}) \\left[ \\hat{l} \\cdot \\hat{n}(\\mathbf{x}) \\right] $$\n",
    "\n",
    "where $\\hat{l}$ is the *light direction* (normalized) and $\\hat{n}(\\mathbf{x})$ is the *surface normal* at location $\\mathbf{x}$.\n",
    "\n",
    "For this breakout session, I have provided you with an *albedo matrix* `albedos` (of dimension $m \\times n$) and the accompanying *normals* `normals` (of dimension $m \\times n \\times 3$, where the normal vector at coordinate `i, j` is `normals[i, j]`).\n",
    "\n",
    "**TASK** Write a function `light_scene` that computes an $m \\times n$ \"lit scene\" image from (1) the albedos, (2) the normals, and (3) the light directions.\n",
    "\n",
    "I have provided you with both data and some partially-implemented functions for this process. If your `light_scene` function is implemented correctly, the `light_scene_interactive` function I have included below should instantiate an interactive widget with which you can move the light around with sliders and see the effect.\n",
    "\n",
    "**PLOTS** Include 4 different lightings of the \"bunny\" scene in your writeup.\n",
    "\n",
    "*Data and Images* from: \n",
    "- Buddah: https://courses.cs.washington.edu/courses/csep576/05wi/projects/project3/project3.htm\n",
    "- Scholar: http://vision.seas.harvard.edu/qsfs/Data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load Data\n",
    "stanford_bunny_dat = np.load('stanford_bunny_dat.npz')\n",
    "normals = stanford_bunny_dat['arr_0'][:, :, :3]\n",
    "albedos = stanford_bunny_dat['arr_0'][:, :, 3]\n",
    "\n",
    "# Function you will be implementing\n",
    "def light_scene(albedos, normals, light_direction):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def visualize_normals(normals, ax=None):\n",
    "    \"\"\"Visualize a 'normals' matrix using common 'look'.\"\"\"\n",
    "    if ax is None:\n",
    "        plt.figure(dpi=300)\n",
    "    plt.imshow(0.5 * normals + 0.5)\n",
    "    \n",
    "def light_scene_interactive(albedos, normals):\n",
    "    # Create a function to generate relit scene and plot\n",
    "    def inter(th_x, th_y):\n",
    "        sth_x = np.sin(th_x)\n",
    "        sth_y = np.sin(th_y)\n",
    "        light_direction = [\n",
    "            sth_x, sth_y, np.sqrt(1 - sth_x**2 - sth_y**2)\n",
    "        ]\n",
    "        ld = light_direction\n",
    "        print(f\"Light Direction: x={ld[0]:0.3f}, y={ld[1]:0.3f}, z={ld[2]:0.3f}\")\n",
    "\n",
    "        plt.figure(2)\n",
    "        imobj = plt.imshow(light_scene(albedos, normals, light_direction),\n",
    "               vmin=0.0, vmax=0.7, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    return interactive(inter, th_x=(-0.8, 0.8, 0.05), th_y=(-0.8, 0.8, 0.05))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(albedos)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "visualize_normals(normals, ax=ax)\n",
    "\n",
    "light_scene_interactive(albedos, normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5.2 Photometric Stereo\n",
    "\n",
    "Now that you see how to light a scene, we will now investigate how to invert this process: a technique known as *photometric stereo*.\n",
    "\n",
    "### P5.2.1 Using provided data\n",
    "\n",
    "I have provided you with some data in the folders `buddah` and `scholar` which contain grayscale `.tga` images and a file `light_directions.txt`. Each line of `light_directions.txt` is a 3-element vector indicating the direction of the light source from the object corresponding to that image. **IMPORTANT:** the light directions for the buddah data are in numerical order of the images. `light_directions[0]` corresponds to `buddah.0.tga` and so on. Sorting by order on some file systems will place `buddah.10.tga` before `buddah.2.tga`: **this is incorrect and will result in incorrect results**.\n",
    "\n",
    "**TASK** Implement photometric stereo as discussed in `L09.2`.\n",
    "\n",
    "**NOTE**: This function may return incorrect values where the albedo is zero. Feel free to add small values to the albedo, or add \"fake\" `[0, 0, 0]` values for the normals where they would otherwise be not well defined.\n",
    "\n",
    "**PLOTS** Plot (1) the computed normals, (2) the computed albedo, and (3) the reconstructed depth image (via the `integrateFrankot` function provided below) for each of the `buddah` and `scholar` datasets.\n",
    "\n",
    "I have included the `integrateFrankot` function below and an example using the \"bunny\" data from above to reconstruct the depth image corresponding to your computed surface normals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You should be using grayscale images for this assignment\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_image_gray(filepath):\n",
    "    \"\"\"Loads an image into a numpy array.\n",
    "    Note: image will have 3 color channels [r, g, b].\"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    img = np.asarray(img).astype(np.float)/255\n",
    "    if len(img.shape) > 2:\n",
    "        return img[:, :, 0]\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The integrateFrankot function is from the CMU Computer Vision course\n",
    "\n",
    "# ##################################################################### #\n",
    "# 16385: Computer Vision Homework 4\n",
    "# Carnegie Mellon University\n",
    "# Spring 2020\n",
    "# ##################################################################### #\n",
    "\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2xyz\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.sparse import kron as spkron\n",
    "from scipy.sparse import eye as speye\n",
    "from scipy.sparse.linalg import lsqr as splsqr\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "\n",
    "def integrateFrankot(normals):\n",
    "\n",
    "    \"\"\"\n",
    "    Implement the Frankot-Chellappa algorithm for enforcing integrability\n",
    "    and normal integration\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    normals: matrix of normal vectors\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    z: numpy.ndarray\n",
    "        The image, of the same size as the derivatives, of estimated depths\n",
    "        at each point\n",
    "\n",
    "    \"\"\"\n",
    "    zx = -normals[:, :, 0]\n",
    "    zy = normals[:, :, 1]\n",
    "    pad = max(normals.shape)\n",
    "\n",
    "    # Raise error if the shapes of the gradients don't match\n",
    "    if not zx.shape == zy.shape:\n",
    "        raise ValueError('Sizes of both gradients must match!')\n",
    "\n",
    "    # Pad the array FFT with a size we specify\n",
    "    h, w = pad, pad\n",
    "\n",
    "    # Fourier transform of gradients for projection\n",
    "    Zx = np.fft.fftshift(np.fft.fft2(zx, (h, w)))\n",
    "    Zy = np.fft.fftshift(np.fft.fft2(zy, (h, w)))\n",
    "    j = 1j\n",
    "\n",
    "    # Frequency grid\n",
    "    [wx, wy] = np.meshgrid(np.linspace(-np.pi, np.pi, w),\n",
    "                           np.linspace(-np.pi, np.pi, h))\n",
    "    absFreq = wx**2 + wy**2\n",
    "\n",
    "    # Perform the actual projection\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        z = (-j*wx*Zx-j*wy*Zy)/absFreq\n",
    "\n",
    "    # Set (undefined) mean value of the surface depth to 0\n",
    "    z[0, 0] = 0.\n",
    "    z = np.fft.ifftshift(z)\n",
    "\n",
    "    # Invert the Fourier transform for the depth\n",
    "    z = np.real(np.fft.ifft2(z))\n",
    "    z = z[:zx.shape[0], :zx.shape[1]]\n",
    "\n",
    "    return z\n",
    "\n",
    "# An example showing how this works with the \"bunny dataset\"\n",
    "normals = np.load('stanford_bunny_normals.npy')\n",
    "depth = integrateFrankot(normals)\n",
    "plt.figure(figsize=(8, 4))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "visualize_normals(normals, ax=ax)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(depth)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5.2.2 Imperfections in Photometric Stereo\n",
    "\n",
    "Using photometric stereo, you have just computed the surface normals and albedo of a few objects. However, for photometric stereo to work, we make some significant assumptions about the structure of the scene and the material properties of the object. In this question, we will use the computed normals and albedos to relight the scene and you will comment on the differences between the \"reconstructed\" image as compared to the original.\n",
    "\n",
    "**TASK** Using your answer to P5.1, relight the scholar object to approximately recreate `scholar/image_11.png` (using `light_positions[10]` from the `scholar` data).\n",
    "\n",
    "**PLOTS*** Plot (1) the original `scholar/image_11.png`, (2) your reconstructed \"relit\" image corresponding to that scene, and (3) the difference between the two images (use `cmap='PiYG', vmin=-0.3, vmax=0.3` as arguments to `imshow`).\n",
    "\n",
    "In ideal photometric stereo, these images should be identical. However, you should notice some differences between your image and the reconstruction, particularly to the lower-left of the face.\n",
    "\n",
    "**QUESTION** Explain (in 2–3 sentences) why there is a significant difference between the two images to the lower-left of the scholar's face.\n",
    "\n",
    "**QUESTION** Provide an explanation (in 2–3 sentences) why the body of the scholar is (on average) darker in the original image than in the reconstruction. (There are a few potential explanations to this question.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5.2.3 Lambertian Objects in Blender\n",
    "\n",
    "I have provided you with a scene in Blender `photometric_stereo.blend` that you will use to generate data of your own. The scene is already arranged so that you can generate data: move around \"Light Source\", render the image, and record the \"light direction\" in `light_positions.txt`. The scene is structured such that simply including the x, y, z coordinates of the light in that file will match the format of the other datasets you have been provided. **Important**: for photometric stereo, the light positions need to be normalized! If you are not normalizing your light directions before computing the albedo and normals, you will need to do so. Also, it is recommended that you move the light only in X and Y, so that you have one fewer variable to worry about.\n",
    "\n",
    "**TASK & PLOTS** Generate at least 4 images using Blender for different light positions and include plots of those images.\n",
    "\n",
    "**PLOTS** Use photometric stereo to compute and plot the (1) albedos, (2) normals, and (3) depth for the blender scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5.2.4 Non-Lambertian Objects in Blender\n",
    "\n",
    "Modify the material of the central object in blender (the \"Icosphere\") so that it has `Metalic` value of `0.5`. The center object should now be shiny.\n",
    "\n",
    "**TASK & PLOTS** Generate at least 4 images using Blender for different light positions and include plots of those images with the new material property.\n",
    "\n",
    "**PLOTS** Use photometric stereo to compute and plot the new (1) albedos, (2) normals, and (3) depth for your modified blender scene.\n",
    "\n",
    "**QUESTION** Explain (2-3 sentences) why the albedo and normals change. Is there a linear solution to finding the correct normals?\n",
    "\n",
    "**QUESTION** Find a research paper (I recommend using Google Scholar) that attempts \"non-lambertian Photometric Stereo\": trying to recover material properties for a non-lamberitan object, like the one shown above. Include a citation of the paper and (in 3–5 sentences) explain how it works and how the approach differs from the lambertian photometric stereo we studied in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P5.3 Structure from Motion\n",
    "\n",
    "The culminating assignment for the second unit of the course is scene reconstruction, also known as *Structure from Motion* (SfM). You will be asked to implement the simplified version of SfM we discussed in class: Affine Structure from Motion, in which the camera is assumed to have an orthographic projection of the scene.\n",
    "\n",
    "### P5.3.1 Implementation\n",
    "\n",
    "The general procedure for Affine SfM was fully derived in class. The general procedure is as follows:\n",
    "\n",
    "<img src=\"images/affine_sfm_procedure.png\" width=\"400\">\n",
    "\n",
    "I have provided you with code that completes the first couple of steps: I have detected features for each of the four provided images and computed feature matches using OpenCV. The feature matches, stored in `combined_matches`, are indexed according to (0) feature number, [1] image number, [2] coordinate. So the x-coordinate of detected feature 5 in image 3, would be `combined_matches[5, 3, 0]`.\n",
    "\n",
    "Second, I have centered the points for each of the images. The resulting centered feature points are stored in the 3-dimensional data structure `matches_norm`. You should use this for constructing the measurement matrix `D`.\n",
    "\n",
    "**TASK** Implement the procedure for Affine Structure from Motion so that you can compute the measurement matrix `M` and the shape matrix `S` as defined in the procedure above.\n",
    "\n",
    "The final step of the algorithm above says to *Eliminate affine ambiguity*; I have provided you with code that does this in `correct_affine_ambiguity`, which takes in the `M` and `S` matrices you are to compute and returns the matrices `A` (containing the affine projection matrices) and `X`.\n",
    "\n",
    "**TASK** Using your newly-computed affine projected matrices (stored in the matrix `A`), you can reproject the 3D points `X` into the image plane. Using the slides from class as a reference, define the function `reproject_image_points(A, X)`, which returns a single matrix `reprojected_image_points` (and has dimensions matching `combined_matches`, with coordinates [0] feature number, [1] image number, [2] coordinate. *Don't forget to re-add the `centers` vector after reprojecting.*\n",
    "\n",
    "**PLOTS** If your functions are all defined correctly, run the code below under `Plotting: Affine SfM`. It should generate a set of 4 plots with the reprojected points overlaid on top of them; include the set of 4 images in your writeup. The four plots should look similar to this:\n",
    "\n",
    "<img src=\"images/affine_sfm_reprojection.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's follow the OpenCV tutorial for F-matrix calculation\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "img1 = cv.imread('SfM/simple_view_c.png',0) #queryimage # center image\n",
    "img2 = cv.imread('SfM/simple_view_1.png',0) #trainimage # left image\n",
    "img3 = cv.imread('SfM/simple_view_2.png',0) #trainimage # upper image\n",
    "img4 = cv.imread('SfM/simple_view_3.png',0) #trainimage # right image\n",
    "sift = cv.SIFT_create()\n",
    "\n",
    "\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath)\n",
    "    return (np.asarray(img).astype(np.float)/255)[:, :, :3]\n",
    "\n",
    "img1_color = load_image('SfM/simple_view_c.png')\n",
    "\n",
    "def get_point_matches(img1, img2):\n",
    "    \"\"\"Returns matches as array: (feature track, image, coord)\"\"\"\n",
    "\n",
    "    # find the keypoints and descriptors with SIFT\n",
    "    kp1, des1 = sift.detectAndCompute(img1,None)\n",
    "    kp2, des2 = sift.detectAndCompute(img2,None)\n",
    "\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 10)\n",
    "    search_params = dict(checks=100)\n",
    "    flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "    matches = flann.knnMatch(des1, des2, k=2)\n",
    "    good = []\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    # ratio test as per Lowe's paper\n",
    "    for i,(m,n) in enumerate(matches):\n",
    "        if m.distance < 0.7*n.distance:\n",
    "            good.append(m)\n",
    "            pts2.append(kp2[m.trainIdx].pt)\n",
    "            pts1.append(kp1[m.queryIdx].pt)\n",
    "        \n",
    "    pts1 = np.int32(pts1)\n",
    "    pts2 = np.int32(pts2)\n",
    "    F, mask = cv.findFundamentalMat(pts1,pts2,cv.FM_LMEDS)\n",
    "    # We select only inlier points\n",
    "    pts1 = pts1[mask.ravel()==1]\n",
    "    pts2 = pts2[mask.ravel()==1]\n",
    "    \n",
    "    return np.stack((pts1, pts2), axis=1)\n",
    "\n",
    "\n",
    "def combine_matches(matches_a, matches_b):\n",
    "    \"\"\"Assumes that the 0'th image is the same between them.\"\"\"\n",
    "    combined_matches = []\n",
    "    for ii in range(matches_a.shape[0]):\n",
    "        ma0 = matches_a[ii, 0]\n",
    "        # Find the match in b\n",
    "        mi = np.where((matches_b[:, 0] == ma0).all(axis=1))[0]\n",
    "\n",
    "        # If a match is found, add to the array\n",
    "        if mi.size > 0:\n",
    "            ma = matches_a[ii]\n",
    "            mb = matches_b[int(mi[0])]\n",
    "            combined_matches.append(np.concatenate(\n",
    "                (ma, mb[1:]), axis=0))\n",
    "\n",
    "    return np.array(combined_matches)\n",
    "\n",
    "\n",
    "def get_match_colors(image_c, combined_matches):\n",
    "    colors = []\n",
    "    nm = combined_matches.shape[0]\n",
    "    for mi in range(nm):\n",
    "        m = combined_matches[mi, 0, :]\n",
    "        colors.append(image_c[m[1]-1:m[1]+2,\n",
    "                              m[0]-1:m[0]+2].sum(axis=0).sum(axis=0)/9)\n",
    "    \n",
    "    return colors\n",
    "\n",
    "\n",
    "# Solve for the Affine Ambiguity\n",
    "def correct_affine_ambiguity(M, S):\n",
    "    Atilde = M\n",
    "    Xtilde = S\n",
    "    m = Atilde.shape[0]//2\n",
    "    Am = np.zeros((3*m, 9))\n",
    "    bm = np.zeros(3*m)\n",
    "    for mi in range(m):\n",
    "        Atl = Atilde[2*mi:2*mi+2]\n",
    "        for ri in range(3):\n",
    "            for ci in range(3):\n",
    "                Am[3*mi+0, 3*ri + ci] = Atl[0, ri] * Atl[0, ci]\n",
    "                bm[3*mi+0] = 1\n",
    "                Am[3*mi+1, 3*ri + ci] = Atl[1, ri] * Atl[1, ci]\n",
    "                bm[3*mi+1] = 1\n",
    "                Am[3*mi+2, 3*ri + ci] = Atl[0, ri] * Atl[1, ci]\n",
    "                bm[3*mi+2] = 0\n",
    "\n",
    "    CCT = np.reshape(np.linalg.lstsq(Am, bm, rcond=None)[0], (3, 3))\n",
    "        \n",
    "    # Enforce a Positive Semi-Definite Matrix\n",
    "    # (usually not necessary)\n",
    "    C = 0.5*(CCT + CCT.T)\n",
    "    w, v = np.linalg.eig(C)\n",
    "    CCT = v @ np.diag(np.abs(w)) @ np.linalg.inv(v)\n",
    "\n",
    "    C = np.linalg.cholesky(CCT)\n",
    "    \n",
    "    A = Atilde @ C\n",
    "    X = np.linalg.inv(C) @ Xtilde\n",
    "    return A, X\n",
    "\n",
    "\n",
    "## Now we compute the feature matches\n",
    "matches_12 = get_point_matches(img1, img2)\n",
    "matches_13 = get_point_matches(img1, img3)\n",
    "matches_14 = get_point_matches(img1, img4)\n",
    "\n",
    "combined_matches = combine_matches(matches_12, matches_13)\n",
    "combined_matches = combine_matches(combined_matches, matches_14)\n",
    "colors = get_match_colors(img1_color, combined_matches)\n",
    "\n",
    "# Plotting code (for visualization)\n",
    "plt.figure(figsize=(12, 3), dpi=150)\n",
    "stacked_images = (img1, img2, img3, img4)\n",
    "num_images = len(stacked_images)\n",
    "for ii in range(num_images):\n",
    "    plt.subplot(1, num_images, ii+1)\n",
    "    plt.imshow(stacked_images[ii], cmap='gray')\n",
    "    plt.plot(combined_matches[:, ii, 0],\n",
    "             combined_matches[:, ii, 1],\n",
    "             '.')\n",
    "\n",
    "    \n",
    "## First we normalize the points\n",
    "# (subtract off average along \"point\" dimension.)\n",
    "centers = np.mean(combined_matches, axis=0)\n",
    "matches_norm = combined_matches - centers\n",
    "\n",
    "\n",
    "## TASK: use matches_norm to solve for the 3D structure\n",
    "# Define M and S from the lecture notes\n",
    "M = None\n",
    "S = None\n",
    "\n",
    "## TASK: \n",
    "def reproject_image_points(A, X, centers):  \n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting: Affine SfM\n",
    "\n",
    "if M is None or S is None:\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "print(f\"M (before correction):\\n {M}\")\n",
    "\n",
    "# Correct for Affine Ambiguity\n",
    "A, X = correct_affine_ambiguity(M, S)\n",
    "\n",
    "print(f\"A (after correction):\\n {A}\")\n",
    "\n",
    "reprojected_image_points = reproject_image_points(A, X, centers)\n",
    "# Save to file for 3D plotting:\n",
    "np.save(\"points.npy\", X)\n",
    "np.save(\"colors.npy\", colors)\n",
    "\n",
    "# Plotting Code (reprojected points):\n",
    "plt.figure(figsize=(8, 8), dpi=150)\n",
    "stacked_images = (img1, img2, img3, img4)\n",
    "num_images = len(stacked_images)\n",
    "for ii in range(num_images):\n",
    "    plt.subplot(2, 2, ii+1)\n",
    "    plt.imshow(stacked_images[ii], cmap='gray')\n",
    "    plt.plot(combined_matches[:, ii, 0],\n",
    "             combined_matches[:, ii, 1],\n",
    "             '.')\n",
    "    plt.plot(reprojected_image_points[:, ii, 0],\n",
    "             reprojected_image_points[:, ii, 1],\n",
    "             'ro', markerfacecolor='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P5.3.2 Followup Questions\n",
    "\n",
    "I have provided you with some plotting code below in `Plotting Code 3D`, which you can use to view your computed 3D points in 3D. **I recommend you run this code in a terminal window, so that you can view it interactively.** The code above saves the data to file in `points.npy` and `colors.npy`, which the plotting code below reads in before visualization. Note also that I have given the features colors according to their color in the original image.\n",
    "\n",
    "**PLOTS** Record 4 different perspectives on your scene using the 3D plotting code. One of those views should show that the points on the surface of the central cube are *coplanar* (i.e. generate a view from above or below the central cube so that you can tell that those points are roughly all on the same plane, as we expect).\n",
    "\n",
    "**QUESTION** What is the relationship between the Affine matrix you have computed and the 3x4 camera projection matrix? Pick one of the input images above. Using the affine transformation matrix you computed for that image, what is the 3x4 camera projection matrix for this image? Include it in your writeup.\n",
    "\n",
    "**QUESTION** I have provided you with the function `correct_affine_ambiguity` so that the Affine projective matrices produced by our algorithm satisfy certain properties. What are these properties? You may refer to the code itself or the course notes in your answer. It may help you to print the matrices before and after the correction is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting Code 3D: Open in interactive terminal so that you can \"look around\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X = np.load(\"points.npy\")\n",
    "colors = np.load(\"colors.npy\")\n",
    "print(colors.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[0, :], X[1, :], X[2, :], c=colors)\n",
    "ax.set_xlim3d([-750, 750])\n",
    "ax.set_ylim3d([-750, 750])\n",
    "ax.set_zlim3d([-750, 750])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (CS682)",
   "language": "python",
   "name": "cs682venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
